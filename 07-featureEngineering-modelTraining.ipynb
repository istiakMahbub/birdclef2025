{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dacc61",
   "metadata": {},
   "source": [
    "In this notebook, I trained a Random Forest classifier using features derived from precomputed log-Mel spectrograms stored as .npz files. These Mel features were extracted from preprocessed audio chunks (already trimmed for human speech in prior steps).\n",
    "\n",
    "Key Steps:\n",
    "- Load metadata from train.csv and map each Mel file to its primary label.\n",
    "- Extract statistical features from each Mel spectrogram:\n",
    "    - Mean and standard deviation across Mel bands (64 + 64 = 128 features per sample).\n",
    "- Label encode species names for model compatibility.\n",
    "- Filter out rare classes (with fewer than 2 samples).\n",
    "- Train-test split using stratified sampling to preserve class distribution.\n",
    "- Train a Random Forest classifier with class balancing to handle label imbalance.\n",
    "- Evaluate the model using accuracy and a classification report.\n",
    "- Export the trained model and label encoder for later inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491238b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6605/6605 [00:08<00:00, 767.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6605 samples with shape 128 features each.\n",
      "\n",
      " Accuracy on test set: 0.7517\n",
      "\n",
      " Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       22973       0.00      0.00      0.00         1\n",
      "       41663       0.89      0.80      0.84        10\n",
      "       50186       1.00      0.90      0.95        10\n",
      "      517119       1.00      0.50      0.67         2\n",
      "      528041       0.00      0.00      0.00         1\n",
      "       52884       1.00      0.50      0.67         2\n",
      "      555086       1.00      1.00      1.00         3\n",
      "       65344       0.00      0.00      0.00         1\n",
      "       65349       1.00      1.00      1.00         1\n",
      "       65448       0.00      0.00      0.00         1\n",
      "       66893       0.00      0.00      0.00         1\n",
      "       67252       1.00      0.67      0.80         3\n",
      "      715170       1.00      1.00      1.00         1\n",
      "     amakin1       0.50      0.50      0.50         2\n",
      "      amekes       0.70      0.54      0.61        13\n",
      "     ampkin1       1.00      0.50      0.67         2\n",
      "      babwar       0.85      0.97      0.90        29\n",
      "     bafibi1       1.00      1.00      1.00         1\n",
      "      banana       0.61      0.85      0.71        26\n",
      "      baymac       1.00      0.75      0.86         8\n",
      "      bbwduc       0.80      0.80      0.80        10\n",
      "     bicwre1       0.86      0.86      0.86         7\n",
      "      bkcdon       1.00      0.25      0.40         4\n",
      "     bkmtou1       0.79      0.85      0.81        13\n",
      "     blbgra1       0.69      0.69      0.69        13\n",
      "     blbwre1       0.83      1.00      0.91         5\n",
      "     blcant4       0.83      0.71      0.77         7\n",
      "     blchaw1       1.00      1.00      1.00         3\n",
      "     blcjay1       0.75      0.80      0.77        15\n",
      "     blhpar1       1.00      0.50      0.67         2\n",
      "      blkvul       0.00      0.00      0.00         1\n",
      "     bobfly1       0.67      0.82      0.73        22\n",
      "     bobher1       0.00      0.00      0.00         1\n",
      "     brtpar1       0.67      0.67      0.67         3\n",
      "     bubcur1       1.00      1.00      1.00         1\n",
      "     bubwre1       0.76      0.72      0.74        18\n",
      "     bucmot3       0.00      0.00      0.00         1\n",
      "      bugtan       0.50      0.30      0.37        10\n",
      "     butsal1       0.80      0.80      0.80        15\n",
      "     cargra1       0.00      0.00      0.00         2\n",
      "      cattyr       0.67      0.67      0.67         3\n",
      "     chbant1       0.71      0.86      0.77        14\n",
      "     chfmac1       1.00      0.67      0.80         6\n",
      "     cinbec1       1.00      0.33      0.50         3\n",
      "     cocwoo1       0.00      0.00      0.00         5\n",
      "     colara1       1.00      0.75      0.86         4\n",
      "     colcha1       0.75      0.43      0.55         7\n",
      "      compau       0.78      0.88      0.83       100\n",
      "     compot1       1.00      0.91      0.95        11\n",
      "     cotfly1       0.00      0.00      0.00         3\n",
      "     crbtan1       1.00      1.00      1.00         2\n",
      "     crcwoo1       1.00      0.88      0.93         8\n",
      "     crebob1       1.00      1.00      1.00         7\n",
      "     cregua1       1.00      1.00      1.00         3\n",
      "     creoro1       0.67      0.93      0.78        15\n",
      "     eardov1       1.00      1.00      1.00         2\n",
      "      fotfly       0.00      0.00      0.00         3\n",
      "     gohman1       0.73      0.69      0.71        16\n",
      "     grasal4       1.00      0.50      0.67         2\n",
      "     grbhaw1       0.83      0.71      0.77         7\n",
      "     greani1       0.75      1.00      0.86         6\n",
      "      greegr       0.88      0.88      0.88         8\n",
      "     greibi1       0.86      1.00      0.92         6\n",
      "      grekis       0.60      0.78      0.68        27\n",
      "     gretin1       0.00      0.00      0.00         2\n",
      "      grnkin       0.82      1.00      0.90         9\n",
      "      gybmar       1.00      1.00      1.00         3\n",
      "     gycwor1       0.58      0.54      0.56        13\n",
      "     labter1       0.00      0.00      0.00         2\n",
      "     laufal1       0.57      0.91      0.70        33\n",
      "      leagre       0.75      1.00      0.86         6\n",
      "     linwoo1       0.90      0.82      0.86        11\n",
      "     littin1       0.84      0.76      0.80        21\n",
      "     mastit1       1.00      0.67      0.80         3\n",
      "      neocor       0.67      1.00      0.80         2\n",
      "     olipic1       0.80      1.00      0.89         4\n",
      "      orcpar       1.00      1.00      1.00         4\n",
      "     palhor2       0.00      0.00      0.00         1\n",
      "     paltan1       1.00      0.43      0.60         7\n",
      "     pavpig2       1.00      0.70      0.82        10\n",
      "     pirfly1       0.57      0.44      0.50         9\n",
      "     piwtyr1       0.00      0.00      0.00         1\n",
      "     plbwoo1       0.88      0.82      0.85        17\n",
      "     plukit1       0.00      0.00      0.00         2\n",
      "     purgal2       0.50      0.60      0.55         5\n",
      "     ragmac1       0.00      0.00      0.00         1\n",
      "     rebbla1       1.00      0.75      0.86         4\n",
      "     recwoo1       0.81      0.93      0.87        14\n",
      "     rinkin1       0.00      0.00      0.00         3\n",
      "      roahaw       0.51      0.65      0.57        31\n",
      "     rosspo1       0.75      0.86      0.80         7\n",
      "     royfly1       0.00      0.00      0.00         1\n",
      "      rtlhum       1.00      0.50      0.67         4\n",
      "     rubsee1       1.00      0.67      0.80         3\n",
      "     rufmot1       0.80      1.00      0.89         4\n",
      "      rugdov       0.62      0.83      0.71         6\n",
      "     rumfly1       0.00      0.00      0.00         3\n",
      "     ruther1       1.00      1.00      1.00         1\n",
      "     rutjac1       0.80      0.44      0.57         9\n",
      "     rutpuf1       0.00      0.00      0.00         1\n",
      "      saffin       0.79      0.73      0.76        15\n",
      "     sahpar1       1.00      1.00      1.00         1\n",
      "     savhaw1       0.00      0.00      0.00         1\n",
      "     secfly1       0.00      0.00      0.00         1\n",
      "     shghum1       0.00      0.00      0.00         1\n",
      "     shtfly1       0.00      0.00      0.00         3\n",
      "      smbani       1.00      0.60      0.75         5\n",
      "      snoegr       0.50      0.67      0.57         3\n",
      "     sobtyr1       0.80      0.71      0.75        17\n",
      "     socfly1       0.81      0.88      0.85        34\n",
      "      solsan       1.00      1.00      1.00         2\n",
      "     soulap1       0.75      0.55      0.63        11\n",
      "     speowl1       0.90      0.95      0.92        19\n",
      "     spepar1       0.88      1.00      0.93         7\n",
      "     srwswa1       1.00      0.67      0.80         3\n",
      "     stbwoo2       1.00      0.50      0.67         6\n",
      "     strcuc1       0.75      0.60      0.67        10\n",
      "     strfly1       0.86      0.82      0.84        22\n",
      "      strher       0.00      0.00      0.00         2\n",
      "     strowl1       0.73      0.73      0.73        11\n",
      "     tbsfin1       1.00      1.00      1.00         2\n",
      "     thbeup1       0.50      0.56      0.53         9\n",
      "     thlsch3       0.80      0.86      0.83        14\n",
      "      trokin       0.67      0.83      0.74        41\n",
      "      tropar       0.75      0.69      0.72        26\n",
      "      trsowl       0.89      0.74      0.81        23\n",
      "      verfly       0.00      0.00      0.00         2\n",
      "     watjac1       1.00      1.00      1.00         2\n",
      "     wbwwre1       0.58      0.68      0.62        28\n",
      "     whbant1       0.91      0.91      0.91        11\n",
      "     whbman1       0.67      0.70      0.68        20\n",
      "     whfant1       0.88      1.00      0.93         7\n",
      "     whmtyr1       1.00      0.67      0.80         3\n",
      "      whtdov       0.67      0.79      0.72        43\n",
      "      y00678       1.00      1.00      1.00         3\n",
      "     yebela1       0.88      0.70      0.78        10\n",
      "     yebfly1       0.80      0.80      0.80        10\n",
      "     yebsee1       0.75      0.60      0.67        20\n",
      "     yecspi2       0.79      0.71      0.75        21\n",
      "     yectyr1       0.00      0.00      0.00         1\n",
      "     yehbla2       1.00      1.00      1.00         1\n",
      "     yehcar1       0.71      0.71      0.71         7\n",
      "     yelori1       0.75      1.00      0.86         3\n",
      "     yeofly1       0.67      0.55      0.60        11\n",
      "     yercac1       0.79      0.90      0.84        30\n",
      "      ywcpar       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.75      1321\n",
      "   macro avg       0.67      0.62      0.63      1321\n",
      "weighted avg       0.74      0.75      0.74      1321\n",
      "\n",
      " Saved model to random_forest_model.pkl and label encoder to label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# === CONFIG ===\n",
    "mel_dir = \"processed/feature\"  # mel feature folder\n",
    "csv_path = \"data/train.csv\"    # train.csv path\n",
    "model_path = \"random_forest_model.pkl\"\n",
    "label_encoder_path = \"label_encoder.pkl\"\n",
    "\n",
    "# === Step 1: Load metadata and label map ===\n",
    "df = pd.read_csv(csv_path)\n",
    "label_map_raw = {\n",
    "    row['filename'].split('/')[-1].split('.')[0]: row['primary_label']\n",
    "    for _, row in df.iterrows()\n",
    "}\n",
    "\n",
    "# Encode string labels as integers\n",
    "le = LabelEncoder()\n",
    "le.fit(list(label_map_raw.values()))\n",
    "label_map = {k: le.transform([v])[0] for k, v in label_map_raw.items()}\n",
    "\n",
    "# === Step 2: Load mel features ===\n",
    "X_raw, y_raw = [], []\n",
    "\n",
    "for fname in tqdm(sorted(os.listdir(mel_dir))):\n",
    "    if not fname.endswith('_mel.npz'):\n",
    "        continue\n",
    "\n",
    "    # Extract clip_id like CSA35130_25_mel.npz → CSA35130\n",
    "    clip_id = fname.rsplit('_', 2)[0]\n",
    "    if clip_id not in label_map:\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(mel_dir, fname)\n",
    "    data = np.load(path)\n",
    "    if 'mel' not in data:\n",
    "        continue\n",
    "\n",
    "    mel = data['mel']\n",
    "    mean = mel.mean(axis=1)\n",
    "    std = mel.std(axis=1)\n",
    "    feature = np.concatenate([mean, std])  # shape: (128,)\n",
    "    X_raw.append(feature)\n",
    "    y_raw.append(label_map[clip_id])\n",
    "\n",
    "X_raw = np.array(X_raw)\n",
    "y_raw = np.array(y_raw)\n",
    "\n",
    "if len(X_raw) == 0:\n",
    "    raise RuntimeError(\"No valid mel files found.\")\n",
    "\n",
    "print(f\"Loaded {X_raw.shape[0]} samples with shape {X_raw.shape[1]} features each.\")\n",
    "\n",
    "# === Step 3: Filter classes with <2 samples ===\n",
    "counts = Counter(y_raw)\n",
    "valid_labels = {label for label, count in counts.items() if count >= 2}\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for xi, yi in zip(X_raw, y_raw):\n",
    "    if yi in valid_labels:\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Step 5: Train Random Forest ===\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Step 6: Evaluate ===\n",
    "y_pred = model.predict(X_test)\n",
    "labels = np.unique(y_test)\n",
    "target_names = [le.inverse_transform([label])[0] for label in labels]\n",
    "\n",
    "report = classification_report(y_test, y_pred, labels=labels, target_names=target_names, zero_division=0)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Accuracy on test set: {accuracy:.4f}\")\n",
    "print(\"\\n Classification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# === Step 7: Save model and label encoder ===\n",
    "joblib.dump(model, model_path)\n",
    "joblib.dump(le, label_encoder_path)\n",
    "print(f\" Saved model to {model_path} and label encoder to {label_encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c07cee",
   "metadata": {},
   "source": [
    "## ⚙️ Environment Requirements\n",
    "\n",
    "To ensure compatibility and avoid serialization issues when saving and loading the `RandomForestClassifier` model, this notebook uses the following library versions:\n",
    "\n",
    "- **scikit-learn**: `1.2.2`  \n",
    "- **numpy**: `1.23.5`\n",
    "\n",
    "These versions match Kaggle's default environment, ensuring that the trained model (`random_forest_model.pkl`) can be loaded successfully during submission.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
