{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dacc61",
   "metadata": {},
   "source": [
    "In this notebook, I trained a Random Forest classifier using features derived from precomputed log-Mel spectrograms stored as .npz files. These Mel features were extracted from preprocessed audio chunks (already trimmed for human speech in prior steps).\n",
    "\n",
    "Key Steps:\n",
    "- Load metadata from train.csv and map each Mel file to its primary label.\n",
    "- Extract statistical features from each Mel spectrogram:\n",
    "    - Mean and standard deviation across Mel bands (64 + 64 = 128 features per sample).\n",
    "- Label encode species names for model compatibility.\n",
    "- Filter out rare classes (with fewer than 2 samples).\n",
    "- Train-test split using stratified sampling to preserve class distribution.\n",
    "- Train a Random Forest classifier with class balancing to handle label imbalance.\n",
    "- Evaluate the model using accuracy and a classification report.\n",
    "- Export the trained model and label encoder for later inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "491238b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121255/121255 [02:27<00:00, 822.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test set: 0.6250\n",
      "\n",
      " Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     1139490       0.00      0.00      0.00         3\n",
      "     1192948       0.57      0.44      0.50         9\n",
      "     1194042       1.00      1.00      1.00         1\n",
      "     1346504       0.71      0.71      0.71         7\n",
      "      134933       1.00      0.40      0.57         5\n",
      "      135045       0.80      0.90      0.85        40\n",
      "     1462711       0.75      0.38      0.50         8\n",
      "     1462737       0.48      0.93      0.64        15\n",
      "       21038       1.00      0.86      0.92         7\n",
      "       21211       0.91      0.69      0.78        29\n",
      "       22333       0.67      0.25      0.36         8\n",
      "       22973       0.90      0.53      0.67        36\n",
      "       22976       0.59      0.56      0.57        18\n",
      "       24272       1.00      1.00      1.00         3\n",
      "       24292       0.50      0.67      0.57         3\n",
      "       24322       1.00      0.86      0.92        14\n",
      "       41663       0.68      0.55      0.61       102\n",
      "       41778       0.83      1.00      0.91         5\n",
      "       41970       1.00      0.83      0.91         6\n",
      "       42007       1.00      0.33      0.50        12\n",
      "       46010       1.00      0.50      0.67         4\n",
      "       47067       0.00      0.00      0.00         2\n",
      "      476537       0.86      0.86      0.86         7\n",
      "      476538       1.00      1.00      1.00         1\n",
      "       48124       0.86      0.80      0.83        64\n",
      "       50186       0.85      0.77      0.81        81\n",
      "      517119       1.00      0.50      0.67        24\n",
      "      528041       0.67      0.40      0.50         5\n",
      "       52884       0.69      0.69      0.69        86\n",
      "      548639       1.00      1.00      1.00         1\n",
      "      555086       0.83      0.73      0.78        26\n",
      "      555142       0.00      0.00      0.00         1\n",
      "      566513       0.83      0.62      0.71        16\n",
      "       64862       1.00      0.85      0.92        13\n",
      "       65336       1.00      0.50      0.67         4\n",
      "       65344       0.90      0.75      0.82        12\n",
      "       65349       0.67      0.40      0.50         5\n",
      "       65373       0.83      0.50      0.62        10\n",
      "       65419       1.00      1.00      1.00         1\n",
      "       65448       0.89      0.59      0.71        27\n",
      "       65547       1.00      0.67      0.80         3\n",
      "       65962       1.00      0.50      0.67         4\n",
      "       66531       1.00      0.50      0.67         2\n",
      "       66578       0.00      0.00      0.00         2\n",
      "       66893       1.00      1.00      1.00         1\n",
      "       67082       1.00      1.00      1.00         1\n",
      "       67252       0.78      0.58      0.67        24\n",
      "      714022       0.55      1.00      0.71         6\n",
      "      715170       0.33      0.56      0.42        18\n",
      "      787625       0.50      0.50      0.50         2\n",
      "      963335       0.38      0.50      0.43         6\n",
      "     amakin1       0.81      0.54      0.65        24\n",
      "      amekes       0.62      0.58      0.60       252\n",
      "     ampkin1       0.90      0.69      0.78        13\n",
      "      anhing       0.78      0.56      0.65        45\n",
      "      babwar       0.76      0.64      0.70       207\n",
      "     bafibi1       0.78      0.70      0.74        10\n",
      "      banana       0.52      0.84      0.64       599\n",
      "      baymac       0.73      0.60      0.66       117\n",
      "      bbwduc       0.74      0.48      0.58       187\n",
      "     bicwre1       0.84      0.60      0.70        62\n",
      "      bkcdon       0.76      0.68      0.72       158\n",
      "     bkmtou1       0.70      0.79      0.74       297\n",
      "     blbgra1       0.68      0.75      0.71       368\n",
      "     blbwre1       0.72      0.63      0.67       161\n",
      "     blcant4       0.78      0.50      0.61        70\n",
      "     blchaw1       0.89      0.48      0.63        33\n",
      "     blcjay1       0.82      0.66      0.73       152\n",
      "     blctit1       0.92      0.63      0.75        19\n",
      "     blhpar1       0.68      0.45      0.54        96\n",
      "      blkvul       0.82      0.54      0.65        57\n",
      "     bobfly1       0.52      0.56      0.54       409\n",
      "     bobher1       0.83      0.29      0.43        17\n",
      "     brtpar1       0.69      0.50      0.58        54\n",
      "     bubcur1       1.00      0.68      0.81        22\n",
      "     bubwre1       0.63      0.60      0.61       337\n",
      "     bucmot3       0.98      0.77      0.86        61\n",
      "      bugtan       0.56      0.37      0.45       210\n",
      "     butsal1       0.69      0.58      0.63       289\n",
      "     cargra1       0.72      0.38      0.50        55\n",
      "      cattyr       0.77      0.61      0.68       122\n",
      "     chbant1       0.70      0.75      0.72       299\n",
      "     chfmac1       0.64      0.53      0.58       110\n",
      "     cinbec1       0.76      0.42      0.54        45\n",
      "     cocher1       0.76      0.81      0.79        16\n",
      "     cocwoo1       0.79      0.66      0.72       165\n",
      "     colara1       0.76      0.47      0.58        47\n",
      "     colcha1       0.61      0.67      0.64       213\n",
      "      compau       0.55      0.86      0.67       782\n",
      "     compot1       0.83      0.78      0.80       317\n",
      "     cotfly1       0.85      0.54      0.66       112\n",
      "     crbtan1       0.93      0.42      0.58        33\n",
      "     crcwoo1       0.79      0.73      0.76       168\n",
      "     crebob1       0.63      0.60      0.62        40\n",
      "     cregua1       0.78      0.63      0.70        75\n",
      "     creoro1       0.67      0.56      0.61       227\n",
      "     eardov1       0.82      0.38      0.52        71\n",
      "      fotfly       0.84      0.42      0.56        64\n",
      "     gohman1       0.77      0.55      0.64       148\n",
      "     grasal4       0.71      0.36      0.48        70\n",
      "     grbhaw1       0.89      0.56      0.68        72\n",
      "     greani1       0.87      0.70      0.78       139\n",
      "      greegr       0.80      0.67      0.73       112\n",
      "     greibi1       0.82      0.61      0.70        74\n",
      "      grekis       0.33      0.63      0.44       671\n",
      "     grepot1       0.98      0.71      0.82        58\n",
      "     gretin1       1.00      0.69      0.82        59\n",
      "      grnkin       0.78      0.45      0.57        80\n",
      "     grysee1       0.70      0.32      0.44        22\n",
      "      gybmar       0.85      0.30      0.45        76\n",
      "     gycwor1       0.58      0.80      0.67       317\n",
      "     labter1       0.80      0.31      0.44        26\n",
      "     laufal1       0.62      0.83      0.71       490\n",
      "      leagre       0.81      0.37      0.51        60\n",
      "     linwoo1       0.70      0.60      0.64       258\n",
      "     littin1       0.74      0.65      0.69       348\n",
      "     mastit1       0.85      0.53      0.65        78\n",
      "      neocor       0.74      0.57      0.65        56\n",
      "     norscr1       0.80      0.63      0.71        19\n",
      "     olipic1       0.87      0.43      0.58        30\n",
      "      orcpar       0.66      0.57      0.61       139\n",
      "     palhor2       0.73      0.32      0.44        25\n",
      "     paltan1       0.72      0.51      0.60       229\n",
      "     pavpig2       0.92      0.51      0.66       114\n",
      "     piepuf1       0.91      0.34      0.50        29\n",
      "     pirfly1       0.72      0.60      0.65       330\n",
      "     piwtyr1       1.00      0.30      0.46        10\n",
      "     plbwoo1       0.87      0.55      0.67       169\n",
      "     plctan1       0.50      0.25      0.33         4\n",
      "     plukit1       0.75      0.51      0.61        35\n",
      "     purgal2       0.85      0.56      0.68        89\n",
      "     ragmac1       1.00      0.25      0.41        55\n",
      "     rebbla1       0.73      0.66      0.69        73\n",
      "     recwoo1       0.82      0.64      0.72        28\n",
      "     rinkin1       0.84      0.35      0.49       107\n",
      "      roahaw       0.35      0.67      0.46       749\n",
      "     rosspo1       0.77      0.63      0.70        38\n",
      "     royfly1       0.92      0.50      0.65        22\n",
      "      rtlhum       0.89      0.68      0.77       149\n",
      "     rubsee1       0.77      0.39      0.52        76\n",
      "     rufmot1       0.84      0.67      0.74       102\n",
      "      rugdov       0.85      0.53      0.65        89\n",
      "     rumfly1       0.87      0.45      0.59       175\n",
      "     ruther1       0.92      0.72      0.81        67\n",
      "     rutjac1       0.79      0.36      0.49       149\n",
      "     rutpuf1       0.88      0.58      0.70        12\n",
      "      saffin       0.54      0.59      0.56       409\n",
      "     sahpar1       1.00      0.38      0.55         8\n",
      "     savhaw1       0.83      0.37      0.51        27\n",
      "     secfly1       0.84      0.56      0.67        82\n",
      "     shghum1       0.57      0.33      0.42        12\n",
      "     shtfly1       0.89      0.47      0.61        66\n",
      "      smbani       0.79      0.52      0.62       161\n",
      "      snoegr       0.81      0.49      0.61        43\n",
      "     sobtyr1       0.62      0.43      0.51       333\n",
      "     socfly1       0.50      0.54      0.52       471\n",
      "      solsan       0.82      0.42      0.55       110\n",
      "     soulap1       0.63      0.39      0.48       189\n",
      "     spbwoo1       0.75      0.35      0.47        26\n",
      "     speowl1       0.77      0.85      0.81       390\n",
      "     spepar1       0.65      0.44      0.52        64\n",
      "     srwswa1       0.76      0.41      0.53        64\n",
      "     stbwoo2       0.82      0.65      0.72       167\n",
      "     strcuc1       0.79      0.57      0.66       355\n",
      "     strfly1       0.76      0.55      0.64       326\n",
      "      strher       0.67      0.58      0.62        50\n",
      "     strowl1       0.88      0.65      0.75       206\n",
      "     tbsfin1       0.91      0.36      0.51        28\n",
      "     thbeup1       0.84      0.46      0.60       166\n",
      "     thlsch3       0.86      0.78      0.81       116\n",
      "      trokin       0.53      0.68      0.60       622\n",
      "      tropar       0.69      0.57      0.62       415\n",
      "      trsowl       0.70      0.80      0.74       569\n",
      "      turvul       0.71      0.45      0.56        11\n",
      "      verfly       0.85      0.35      0.50        48\n",
      "     watjac1       0.83      0.49      0.62        49\n",
      "     wbwwre1       0.58      0.59      0.59       461\n",
      "     whbant1       0.80      0.53      0.64       104\n",
      "     whbman1       0.60      0.62      0.61       320\n",
      "     whfant1       0.69      0.55      0.61       129\n",
      "     whmtyr1       0.93      0.42      0.58        31\n",
      "      whtdov       0.51      0.81      0.62       734\n",
      "     whttro1       0.97      0.53      0.69        60\n",
      "     whwswa1       0.85      0.56      0.68        41\n",
      "      woosto       0.78      0.75      0.76        28\n",
      "      y00678       0.81      0.39      0.53        87\n",
      "     yebela1       0.86      0.40      0.55       149\n",
      "     yebfly1       0.79      0.55      0.65       162\n",
      "     yebsee1       0.77      0.56      0.65       198\n",
      "     yecspi2       0.67      0.70      0.68       245\n",
      "     yectyr1       0.75      0.28      0.41        53\n",
      "     yehbla2       0.81      0.65      0.72        68\n",
      "     yehcar1       0.79      0.42      0.55       143\n",
      "     yelori1       0.91      0.60      0.73        53\n",
      "     yeofly1       0.57      0.62      0.59       530\n",
      "     yercac1       0.42      0.70      0.53       529\n",
      "      ywcpar       0.84      0.58      0.69       139\n",
      "\n",
      "    accuracy                           0.63     24251\n",
      "   macro avg       0.76      0.57      0.63     24251\n",
      "weighted avg       0.67      0.63      0.63     24251\n",
      "\n",
      " Saved model to random_forest_model.pkl and label encoder to label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# === CONFIG ===\n",
    "mel_dir = \"processed/feature\"  # mel feature folder\n",
    "csv_path = \"data/train.csv\"    # train.csv path\n",
    "model_path = \"random_forest_model.pkl\"\n",
    "label_encoder_path = \"label_encoder.pkl\"\n",
    "\n",
    "# === Step 1: Load metadata and label map ===\n",
    "\"\"\"\n",
    "The goal of Step 1 is to read the metadata (CSV file) containing filenames and their corresponding labels,\n",
    "and then map these labels (bird species) from strings to integers using LabelEncoder. \n",
    "This is necessary for later stages in machine learning, where the model requires numerical input and output.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "label_map_raw = {\n",
    "    row['filename'].split('/')[-1].split('.')[0]: row['primary_label']\n",
    "    for _, row in df.iterrows()\n",
    "}\n",
    "\n",
    "# Encode string labels as integers\n",
    "le = LabelEncoder()\n",
    "le.fit(list(label_map_raw.values()))\n",
    "label_map = {k: le.transform([v])[0] for k, v in label_map_raw.items()}\n",
    "\n",
    "# === Step 2: Load mel features ===\n",
    "\"\"\"\n",
    "In this step, the code loads the mel spectrograms (spectral features) for each audio file\n",
    "and processes them by computing the mean and standard deviation for each mel frequency band.\n",
    "\"\"\"\n",
    "\n",
    "X_raw, y_raw = [], []\n",
    "\n",
    "for fname in tqdm(sorted(os.listdir(mel_dir))):\n",
    "    if not fname.endswith('_mel.npz'):\n",
    "        continue\n",
    "\n",
    "    # Extract clip_id like CSA35130_25_mel.npz → CSA35130\n",
    "    clip_id = fname.rsplit('_', 2)[0]\n",
    "    if clip_id not in label_map:\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(mel_dir, fname)\n",
    "    data = np.load(path)\n",
    "    if 'mel' not in data:\n",
    "        continue\n",
    "\n",
    "    mel = data['mel']\n",
    "    mean = mel.mean(axis=1)\n",
    "    std = mel.std(axis=1)\n",
    "    feature = np.concatenate([mean, std])  # shape: (128,)\n",
    "    X_raw.append(feature)\n",
    "    y_raw.append(label_map[clip_id])\n",
    "\n",
    "X_raw = np.array(X_raw)\n",
    "y_raw = np.array(y_raw)\n",
    "\n",
    "if len(X_raw) == 0:\n",
    "    raise RuntimeError(\"No valid mel files found.\")\n",
    "\n",
    "#print(f\"Loaded {X_raw.shape[0]} samples with shape {X_raw.shape[1]} features each.\")\n",
    "\n",
    "# === Step 3: Filter classes with <2 samples ===\n",
    "counts = Counter(y_raw)\n",
    "valid_labels = {label for label, count in counts.items() if count >= 2}\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for xi, yi in zip(X_raw, y_raw):\n",
    "    if yi in valid_labels:\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# === Step 5: Train Random Forest ===\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Step 6: Evaluate ===\n",
    "y_pred = model.predict(X_test)\n",
    "labels = np.unique(y_test)\n",
    "target_names = [le.inverse_transform([label])[0] for label in labels]\n",
    "\n",
    "report = classification_report(y_test, y_pred, labels=labels, target_names=target_names, zero_division=0)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n Accuracy on test set: {accuracy:.4f}\")\n",
    "print(\"\\n Classification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# === Step 7: Save model and label encoder ===\n",
    "joblib.dump(model, model_path)\n",
    "joblib.dump(le, label_encoder_path)\n",
    "print(f\" Saved model to {model_path} and label encoder to {label_encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c07cee",
   "metadata": {},
   "source": [
    "## ⚙️ Environment Requirements\n",
    "\n",
    "To ensure compatibility and avoid serialization issues when saving and loading the `RandomForestClassifier` model, this notebook uses the following library versions:\n",
    "\n",
    "- **scikit-learn**: `1.2.2`  \n",
    "- **numpy**: `1.23.5`\n",
    "\n",
    "These versions match Kaggle's default environment, ensuring that the trained model (`random_forest_model.pkl`) can be loaded successfully during submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01691635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Load the label encoder\n",
    "le = joblib.load(\"label_encoder_v2.pkl\")\n",
    "\n",
    "# Count number of unique classes\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdclef_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
